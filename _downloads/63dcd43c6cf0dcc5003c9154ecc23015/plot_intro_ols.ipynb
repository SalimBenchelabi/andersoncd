{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of CD, GD, inertial and Anderson acceleration\n\nCoordinate descent outperforms gradient descent, and Anderson acceleration\noutperforms inertial acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import norm\nfrom scipy.sparse.linalg import cg\nfrom libsvmdata import fetch_libsvm\nfrom andersoncd.plot_utils import configure_plt, _plot_legend_apart\n\nfrom andersoncd.lasso import solver_enet, primal_enet, apcg_enet\n\n\nconfigure_plt()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_features = 1000\nX, y = fetch_libsvm('rcv1.binary', normalize=True)\nX = X[:, :n_features]\n\ny -= y.mean()\ny /= norm(y)\n\n# conjugate gradient competitor:\nE_cg = []\nE_cg.append(norm(y) ** 2 / 2)\n\n\ndef callback(x):\n    pobj = primal_enet(y - X @ x, x, 0)\n    E_cg.append(pobj)\n\n\nw_star = cg(\n    X.T @ X, X.T @ y, callback=callback, maxiter=500, tol=1e-32)[0]\nE_cg = np.array(E_cg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run algorithms:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# solvers parameters:\nalpha = 0  # Least Squares\ntol = 1e-13\nmax_iter = 2000\nf_gap = 10\n\nall_algos = [\n    ('pgd', False),\n    ('cd', False),\n    ('pgd', True),\n    ('cd', True),\n    ('fista', False),\n    ('apcg', False)\n]\n\ndict_algo_name = {}\ndict_algo_name[\"pgd\", False] = \"GD\"\ndict_algo_name[\"cd\", False] = \"CD\"\ndict_algo_name[\"pgd\", True] = \"GD - Anderson\"\ndict_algo_name[\"cd\", True] = \"CD - Anderson\"\ndict_algo_name[\"fista\", False] = \"GD - inertial\"\ndict_algo_name[\"apcg\", False] = \"CD - inertial\"\n\n\ndict_Es = {}\n\nfor algo in all_algos:\n    print(\"Running \", dict_algo_name[algo])\n    if algo[0] == 'apcg':\n        w, E, gaps = apcg_enet(\n            X, y, alpha, max_iter=max_iter, tol=tol, f_gap=f_gap,\n            verbose=False)\n    else:\n        w, E, _ = solver_enet(\n            X, y, alpha=alpha, f_gap=f_gap, max_iter=max_iter, tol=tol,\n            algo=algo[0], use_acc=algo[1], verbose=False)\n    dict_Es[algo] = E.copy()\n\n\ncurrent_palette = sns.color_palette(\"colorblind\")\ndict_color = {}\ndict_color[\"pgd\"] = current_palette[0]\ndict_color[\"fista\"] = current_palette[0]\ndict_color[\"cd\"] = current_palette[1]\ndict_color[\"apcg\"] = current_palette[1]\n\n\np_star = primal_enet(y - X @ w_star, w_star, alpha)\nfor E in dict_Es.values():\n    p_star = min(p_star, min(E))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot convergence curves:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "save_fig = False\n# save_fig = True\nif save_fig:\n    figsize = (8, 3)\nelse:\n    figsize = (10, 5)\n\nplt.close('all')\nfig, ax = plt.subplots(figsize=figsize)\n\n\nfor i, algo in enumerate(all_algos):\n    E = dict_Es[algo]\n    use_acc = algo[1]\n    if use_acc:\n        linestyle = 'dashed'\n    elif algo[0].startswith(('fista', 'apcg')):\n        linestyle = 'dotted'\n    else:\n        linestyle = 'solid'\n\n    if i == 2:\n        ax.semilogy(\n            np.arange(len(E_cg)), E_cg - p_star, label=\"conjugate grad.\",\n            color='black', linestyle='dashdot')\n    ax.semilogy(\n        f_gap * np.arange(len(E)), E - p_star,\n        label=dict_algo_name[algo],\n        color=dict_color[algo[0]], linestyle=linestyle)\n\n\nplt.ylabel(r\"$f(x^{(k)}) - f(x^{*})$\")\nplt.xlabel(r\"iteration $k$\")\nax.set_yticks((1e-15, 1e-10, 1e-5, 1e0))\nplt.tight_layout()\n\n\nfig_dir = \"../\"\nfig_dir_svg = \"../\"\n\nif save_fig:\n    fig.savefig(\n        \"%sintro_ols.pdf\" % fig_dir, bbox_inches=\"tight\")\n    fig.savefig(\n        \"%sintro_ols.svg\" % fig_dir_svg, bbox_inches=\"tight\")\n    fig = _plot_legend_apart(\n        ax, \"%sintro_ols_legend.pdf\" % fig_dir, ncol=3)\n\nplt.title(\"Convergence on Least Squares\")\nplt.legend()\nplt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}