{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of CD, GD, inertial and Anderson acceleration\n\nCoordinate descent outperforms gradient descent, and Anderson acceleration\noutperforms inertial acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import sparse\nfrom numpy.linalg import norm\nfrom scipy.sparse.linalg import cg\nfrom libsvmdata import fetch_libsvm\nfrom andersoncd.plot_utils import configure_plt\n\nfrom andersoncd.lasso import solver_enet, primal_enet, apcg\n\n\nconfigure_plt()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_features = 1000\nX, y = fetch_libsvm('rcv1_train')\nX = X[:, :n_features]\n\nX.multiply(1 / sparse.linalg.norm(X, axis=0))\ny -= y.mean()\ny /= norm(y)\n\n# conjugate gradient competitor:\nE_cg = []\nE_cg.append(norm(y) ** 2 / 2)\n\n\ndef callback(x):\n    pobj = primal_enet(y - X @ x, x, 0)\n    E_cg.append(pobj)\n\n\nw_star = cg(\n    X.T @ X, X.T @ y, callback=callback, maxiter=500, tol=1e-32)[0]\nE_cg = np.array(E_cg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run algorithms:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# solvers parameters:\nalpha = 0  # Least Squares\ntol = 1e-13\nmax_iter = 2000\nf_gap = 10\n\nall_algos = [\n    ('pgd', False),\n    ('pgd', True),\n    ('fista', False),\n    ('cd', False),\n    ('cd', True),\n    ('apcg', False),\n]\n\ndict_algo_name = {}\ndict_algo_name[\"pgd\", False] = \"GD\"\ndict_algo_name[\"cd\", False] = \"CD\"\ndict_algo_name[\"pgd\", True] = \"GD - Anderson\"\ndict_algo_name[\"cd\", True] = \"CD - Anderson\"\ndict_algo_name[\"fista\", False] = \"GD - inertial\"\ndict_algo_name[\"apcg\", False] = \"CD - inertial\"\n\n\ndict_Es = {}\n\nfor algo in all_algos:\n    print(\"Running \", dict_algo_name[algo])\n    if algo[0] == 'apcg':\n        w, E, gaps = apcg(\n            X, y, alpha, max_iter=max_iter, tol=tol, f_gap=f_gap,\n            verbose=False)\n    else:\n        w, E, _ = solver_enet(\n            X, y, alpha=alpha, f_gap=f_gap, max_iter=max_iter, tol=tol,\n            algo=algo[0], use_acc=algo[1], verbose=False)\n    dict_Es[algo] = E.copy()\n\n\ncurrent_palette = sns.color_palette(\"colorblind\")\ndict_color = {}\ndict_color[\"pgd\"] = current_palette[0]\ndict_color[\"fista\"] = current_palette[0]\ndict_color[\"cd\"] = current_palette[1]\ndict_color[\"apcg\"] = current_palette[1]\n\n\np_star = primal_enet(y - X @ w_star, w_star, alpha)\nfor E in dict_Es.values():\n    p_star = min(p_star, min(E))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot convergence curves:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.close('all')\nfig, ax = plt.subplots(figsize=(9, 6))\n\n\nfor algo in all_algos:\n    E = dict_Es[algo]\n    use_acc = algo[1]\n    if use_acc:\n        linestyle = 'dashed'\n    elif algo[0].startswith(('fista', 'apcg')):\n        linestyle = 'dotted'\n    else:\n        linestyle = 'solid'\n\n    ax.semilogy(\n        f_gap * np.arange(len(E)), E - p_star,\n        label=dict_algo_name[algo],\n        color=dict_color[algo[0]], linestyle=linestyle)\n\nax.semilogy(\n    np.arange(len(E_cg)), E_cg - p_star, label=\"conjugate grad.\",\n    color='black', linestyle='dashdot')\n\nplt.ylabel(r\"$f(x^{(k)}) - f(x^{*})$\")\nplt.xlabel(r\"iteration $k$\")\nax.set_yticks((1e-15, 1e-10, 1e-5, 1e0))\nplt.title(\"Convergence on Least Squares\")\nplt.tight_layout()\n\n\nplt.legend()\nplt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}