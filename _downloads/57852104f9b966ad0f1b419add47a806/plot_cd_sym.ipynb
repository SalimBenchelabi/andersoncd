{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Convergence of CD, pseudo-symmetric CD, and their Anderson versions\n\nOn least squares and logistic regression, performance of pseudo-symmetric\ncoordinate descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n\nimport numpy as np\nimport seaborn as sns\nfrom scipy import sparse\nfrom numpy.linalg import norm\nimport matplotlib.pyplot as plt\nfrom scipy.sparse.linalg import cg\nfrom scipy.optimize import fmin_l_bfgs_b\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import label_binarize\nfrom celer.datasets import fetch_libsvm\nfrom celer.plot_utils import configure_plt\n\nfrom andersoncd.utils import power_method\nfrom andersoncd.lasso import solver_enet, primal_enet\nfrom andersoncd.logreg import solver_logreg, primal_logreg\n\n\nconfigure_plt()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = \"real-sim\"\nn_features = 1000\nX, y = fetch_libsvm(dataset)\n\nX = X[:, :n_features]\nX.multiply(1 / sparse.linalg.norm(X, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate figures for both Least Squares and Logistic regression\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for pb in (\"ols\", \"logreg\"):\n    if pb == 'lasso':\n        y -= y.mean()\n        y /= norm(y)\n\n    f_gap = 10\n    tol = 1e-15\n    max_iter = 2000\n\n    # run \"best algorithm\": conj. grad. for LS, LBFGS for logreg:\n    E_optimal = []\n    if pb == \"logreg\":\n        rho = power_method(X) ** 2 / 100_000  # a bit of enet regularization\n        E_optimal.append(np.log(2) * len(y))\n        label_opt = \"L-BFGS\"\n\n        def callback(x):\n            pobj = primal_logreg(X @ x, y, x, 0, rho)\n            E_optimal.append(pobj)\n\n        def obj(x):\n            return np.log(1 + np.exp(- y * (X @ x))).sum() + rho * x @ x / 2\n\n        def fprime(x):\n            return - X.T @ (y / (1 + np.exp(y * (X @ x)))) + rho * x\n\n        fmin_l_bfgs_b(obj, np.zeros(\n            X.shape[1]), fprime=fprime, callback=callback, factr=0.01, pgtol=0,\n            maxiter=max_iter)\n    else:\n        alpha = 0\n        rho = 0  # no elastic net\n        E_optimal.append(norm(y) ** 2 / 2)\n        label_opt = \"conjugate grad.\"\n\n        def callback(x):\n            pobj = primal_enet(y - X @ x, x, alpha)\n            E_optimal.append(pobj)\n        cg(X.T @ X, X.T @ y, callback=callback, maxiter=max_iter, tol=1e-32)\n    E_optimal = np.array(E_optimal)\n\n    all_algos = [\n        ('cd', False),\n        ('cd', True),\n        ('cdsym', False),\n        ('cdsym', True),\n    ]\n\n    dict_coef = defaultdict(lambda: 1)\n    dict_coef['cdsym'] = 2\n    algo_names = {}\n    algo_names[\"cd\", False] = \"CD\"\n    algo_names[\"cdsym\", False] = \"CD sym\"\n    algo_names[\"cd\", True] = \"CD - Anderson\"\n    algo_names[\"cdsym\", True] = \"CD sym - Anderson\"\n\n    dict_Es = {}\n\n    for algo in all_algos:\n        print(\"Running %s\" % algo_names[algo])\n        if pb == \"ols\":\n            w, E, _ = solver_enet(\n                X, y, alpha=alpha, f_gap=f_gap,\n                max_iter=int(max_iter/dict_coef[algo[0]]), tol=tol,\n                algo=algo[0], use_acc=algo[1])\n        elif pb == \"logreg\":\n            w, E, _ = solver_logreg(\n                X, y, alpha=alpha, rho=rho, f_gap=f_gap,\n                max_iter=max_iter//dict_coef[algo[0]], tol=tol,\n                algo=algo[0], use_acc=algo[1])\n        dict_Es[algo] = E\n\n    current_palette = sns.color_palette(\"colorblind\")\n    dict_color = {}\n    dict_color[\"cd\"] = current_palette[1]\n    dict_color['cdsym'] = current_palette[2]\n\n    p_star = E_optimal[-1]\n    for E in dict_Es.values():\n        p_star = min(p_star, min(E))\n\n    plt.close('all')\n    fig, ax = plt.subplots(figsize=(10, 5))\n    for i, algo in enumerate(all_algos):\n        E = dict_Es[algo]\n        use_acc = algo[1]\n        linestyle = 'dashed' if use_acc else 'solid'\n\n        ax.semilogy(\n            dict_coef[algo[0]] * f_gap * np.arange(len(E)), E - p_star,\n            label=algo_names[algo],\n            color=dict_color[algo[0]], linestyle=linestyle)\n\n    ax.semilogy(\n        np.arange(len(E_optimal)), E_optimal - p_star,\n        label=label_opt, color='black', linestyle='dashdot')\n\n    dict_dataset = {}\n    dict_dataset[\"rcv1_train\"] = \"rcv1\"\n    dict_dataset[\"real-sim\"] = \"real_sim\"  # use _ not - for latex\n    dict_dataset[\"leukemia\"] = \"leukemia\"\n\n    str_info = \"%s (%i st columns)\" % (dataset, n_features)\n    title = pb + str_info\n\n    plt.ylabel(r\"$f(x^{(k)}) - f(x^{*})$\")\n    plt.xlabel(\"nb gradient calls\")\n    plt.ylim((1e-10, None))\n    plt.tight_layout()\n\n    plt.legend()\n    plt.title(title.replace('_', ' '))\n    plt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}