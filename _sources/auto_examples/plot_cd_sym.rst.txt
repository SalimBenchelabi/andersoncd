.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_cd_sym.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_plot_cd_sym.py:


Convergence of CD, pseudo-symmetric CD, and their Anderson versions
==================================================================

On least squares and logistic regression, performance of pseudo-symmetric
coordinate descent.


.. code-block:: default

    from collections import defaultdict

    import numpy as np
    import seaborn as sns
    from scipy import sparse
    from numpy.linalg import norm
    import matplotlib.pyplot as plt
    from scipy.sparse.linalg import cg
    from scipy.optimize import fmin_l_bfgs_b
    from sklearn.datasets import fetch_openml
    from sklearn.preprocessing import label_binarize
    from celer.datasets import fetch_libsvm
    from celer.plot_utils import configure_plt

    from andersoncd.utils import power_method
    from andersoncd.lasso import solver_enet, primal_enet
    from andersoncd.logreg import solver_logreg, primal_logreg


    configure_plt()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    usetex mode requires TeX.




Load the data:


.. code-block:: default

    dataset = "real-sim"
    n_features = 1000
    X, y = fetch_libsvm(dataset)

    X = X[:, :n_features]
    X.multiply(1 / sparse.linalg.norm(X, axis=0))






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Dataset: real-sim
    Downloading data from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/real-sim.bz2 (33.6 MB)

    file_sizes:   0%|                                   | 0.00/35.2M [00:00<?, ?B/s]    file_sizes:   0%|                           | 24.6k/35.2M [00:00<05:29, 107kB/s]    file_sizes:   0%|                           | 49.2k/35.2M [00:00<05:39, 104kB/s]    file_sizes:   0%|                            | 106k/35.2M [00:00<04:39, 126kB/s]    file_sizes:   1%|1                           | 221k/35.2M [00:00<03:36, 162kB/s]    file_sizes:   1%|3                           | 451k/35.2M [00:01<02:40, 216kB/s]    file_sizes:   3%|7                           | 909k/35.2M [00:01<01:56, 295kB/s]    file_sizes:   5%|#4                         | 1.83M/35.2M [00:01<01:21, 408kB/s]    file_sizes:  10%|##8                        | 3.66M/35.2M [00:01<00:55, 569kB/s]    file_sizes:  15%|####                       | 5.23M/35.2M [00:02<00:38, 784kB/s]    file_sizes:  19%|#####                     | 6.81M/35.2M [00:02<00:26, 1.07MB/s]    file_sizes:  24%|######1                   | 8.38M/35.2M [00:02<00:18, 1.43MB/s]    file_sizes:  28%|#######3                  | 9.95M/35.2M [00:02<00:13, 1.87MB/s]    file_sizes:  33%|########5                 | 11.5M/35.2M [00:03<00:09, 2.39MB/s]    file_sizes:  37%|#########6                | 13.1M/35.2M [00:03<00:07, 2.96MB/s]    file_sizes:  43%|###########2              | 15.2M/35.2M [00:03<00:05, 3.67MB/s]    file_sizes:  48%|############3             | 16.8M/35.2M [00:03<00:04, 4.25MB/s]    file_sizes:  52%|#############5            | 18.3M/35.2M [00:04<00:03, 4.79MB/s]    file_sizes:  57%|##############7           | 19.9M/35.2M [00:04<00:02, 5.25MB/s]    file_sizes:  61%|###############8          | 21.5M/35.2M [00:04<00:02, 5.64MB/s]    file_sizes:  65%|#################         | 23.1M/35.2M [00:04<00:02, 5.94MB/s]    file_sizes:  70%|##################1       | 24.6M/35.2M [00:04<00:01, 6.17MB/s]    file_sizes:  74%|###################3      | 26.2M/35.2M [00:05<00:01, 6.35MB/s]    file_sizes:  79%|####################5     | 27.8M/35.2M [00:05<00:01, 6.47MB/s]    file_sizes:  83%|#####################6    | 29.4M/35.2M [00:05<00:00, 6.56MB/s]    file_sizes:  88%|######################8   | 30.9M/35.2M [00:05<00:00, 6.63MB/s]    file_sizes:  91%|#######################6  | 32.0M/35.2M [00:06<00:00, 5.92MB/s]    file_sizes:  95%|########################5 | 33.3M/35.2M [00:06<00:00, 5.88MB/s]    file_sizes: 100%|#########################9| 35.1M/35.2M [00:06<00:00, 6.34MB/s]    file_sizes: 100%|##########################| 35.2M/35.2M [00:06<00:00, 5.37MB/s]
    Successfully downloaded file to /home/circleci/celer_data/binary/real-sim.bz2
    Decompressing...
    Loading svmlight file...

    <72309x1000 sparse matrix of type '<class 'numpy.float64'>'
    	with 1124310 stored elements in COOrdinate format>



Generate figures for both Least Squares and Logistic regression


.. code-block:: default

    for pb in ("ols", "logreg"):
        if pb == 'lasso':
            y -= y.mean()
            y /= norm(y)

        f_gap = 10
        tol = 1e-15
        max_iter = 2000

        # run "best algorithm": conj. grad. for LS, LBFGS for logreg:
        E_optimal = []
        if pb == "logreg":
            rho = power_method(X) ** 2 / 100_000  # a bit of enet regularization
            E_optimal.append(np.log(2) * len(y))
            label_opt = "L-BFGS"

            def callback(x):
                pobj = primal_logreg(X @ x, y, x, 0, rho)
                E_optimal.append(pobj)

            def obj(x):
                return np.log(1 + np.exp(- y * (X @ x))).sum() + rho * x @ x / 2

            def fprime(x):
                return - X.T @ (y / (1 + np.exp(y * (X @ x)))) + rho * x

            fmin_l_bfgs_b(obj, np.zeros(
                X.shape[1]), fprime=fprime, callback=callback, factr=0.01, pgtol=0,
                maxiter=max_iter)
        else:
            alpha = 0
            rho = 0  # no elastic net
            E_optimal.append(norm(y) ** 2 / 2)
            label_opt = "conjugate grad."

            def callback(x):
                pobj = primal_enet(y - X @ x, x, alpha)
                E_optimal.append(pobj)
            cg(X.T @ X, X.T @ y, callback=callback, maxiter=max_iter, tol=1e-32)
        E_optimal = np.array(E_optimal)

        all_algos = [
            ('cd', False),
            ('cd', True),
            ('cdsym', False),
            ('cdsym', True),
        ]

        dict_coef = defaultdict(lambda: 1)
        dict_coef['cdsym'] = 2
        algo_names = {}
        algo_names["cd", False] = "CD"
        algo_names["cdsym", False] = "CD sym"
        algo_names["cd", True] = "CD - Anderson"
        algo_names["cdsym", True] = "CD sym - Anderson"

        dict_Es = {}

        for algo in all_algos:
            print("Running %s" % algo_names[algo])
            if pb == "ols":
                w, E, _ = solver_enet(
                    X, y, alpha=alpha, f_gap=f_gap,
                    max_iter=int(max_iter/dict_coef[algo[0]]), tol=tol,
                    algo=algo[0], use_acc=algo[1])
            elif pb == "logreg":
                w, E, _ = solver_logreg(
                    X, y, alpha=alpha, rho=rho, f_gap=f_gap,
                    max_iter=max_iter//dict_coef[algo[0]], tol=tol,
                    algo=algo[0], use_acc=algo[1])
            dict_Es[algo] = E

        current_palette = sns.color_palette("colorblind")
        dict_color = {}
        dict_color["cd"] = current_palette[1]
        dict_color['cdsym'] = current_palette[2]

        p_star = E_optimal[-1]
        for E in dict_Es.values():
            p_star = min(p_star, min(E))

        plt.close('all')
        fig, ax = plt.subplots(figsize=(10, 5))
        for i, algo in enumerate(all_algos):
            E = dict_Es[algo]
            use_acc = algo[1]
            linestyle = 'dashed' if use_acc else 'solid'

            ax.semilogy(
                dict_coef[algo[0]] * f_gap * np.arange(len(E)), E - p_star,
                label=algo_names[algo],
                color=dict_color[algo[0]], linestyle=linestyle)

        ax.semilogy(
            np.arange(len(E_optimal)), E_optimal - p_star,
            label=label_opt, color='black', linestyle='dashdot')

        dict_dataset = {}
        dict_dataset["rcv1_train"] = "rcv1"
        dict_dataset["real-sim"] = "real_sim"  # use _ not - for latex
        dict_dataset["leukemia"] = "leukemia"

        str_info = "%s (%i st columns)" % (dataset, n_features)
        title = pb + str_info

        plt.ylabel(r"$f(x^{(k)}) - f(x^{*})$")
        plt.xlabel("nb gradient calls")
        plt.ylim((1e-10, None))
        plt.tight_layout()

        plt.legend()
        plt.title(title.replace('_', ' '))
        plt.show(block=False)



.. image:: /auto_examples/images/sphx_glr_plot_cd_sym_001.png
    :alt: logregreal-sim (1000 st columns)
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Running CD
    Running CD - Anderson
    Running CD sym
    Running CD sym - Anderson
    Running CD
    Running CD - Anderson
    Running CD sym
    Running CD sym - Anderson





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 5 minutes  11.546 seconds)


.. _sphx_glr_download_auto_examples_plot_cd_sym.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_cd_sym.py <plot_cd_sym.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_cd_sym.ipynb <plot_cd_sym.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
